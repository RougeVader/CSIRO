{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "from pathlib import Path\n",
    "import pickle\n",
    "from keras.models import load_model\n",
    "from keras.applications.vgg16 import VGG16, preprocess_input\n",
    "from keras.preprocessing import image\n",
    "from sklearn.ensemble import RandomForestRegressor\n",
    "from sklearn.preprocessing import OneHotEncoder\n",
    "\n",
    "# Load the data and models\n",
    "data_path = Path('.')\n",
    "train_df = pd.read_csv(data_path / 'train.csv')\n",
    "test_df = pd.read_csv(data_path / 'test.csv')\n",
    "metadata_model = load_model(data_path / 'metadata_model.h5')\n",
    "with open(data_path / 'state_encoder.pkl', 'rb') as f:\n",
    "    state_encoder = pickle.load(f)\n",
    "with open(data_path / 'species_encoder.pkl', 'rb') as f:\n",
    "    species_encoder = pickle.load(f)\n",
    "\n",
    "# Function to predict metadata\n",
    "def predict_metadata(img_path):\n",
    "    img = image.load_img(img_path, target_size=(224, 224))\n",
    "    x = image.img_to_array(img)\n",
    "    x = np.expand_dims(x, axis=0)\n",
    "    x = x / 255.0\n",
    "    state_pred, species_pred = metadata_model.predict(x)\n",
    "    state = state_encoder.inverse_transform([np.argmax(state_pred)])[0]\n",
    "    species = species_encoder.inverse_transform([np.argmax(species_pred)])[0]\n",
    "    return state, species\n",
    "\n",
    "# Predict metadata for the test set\n",
    "test_metadata = test_df['image_path'].apply(lambda x: predict_metadata(data_path / x))\n",
    "test_df[['State', 'Species']] = pd.DataFrame(test_metadata.tolist(), index=test_df.index)\n",
    "\n",
    "# Pivot the training data\n",
    "train_pivot = train_df.pivot_table(index=['image_path', 'Sampling_Date', 'State', 'Species', 'Pre_GSHH_NDVI', 'Height_Ave_cm'], columns='target_name', values='target').reset_index()\n",
    "\n",
    "# Feature Engineering\n",
    "train_pivot['Sampling_Date'] = pd.to_datetime(train_pivot['Sampling_Date'])\n",
    "train_pivot['month'] = train_pivot['Sampling_Date'].dt.month\n",
    "train_pivot['year'] = train_pivot['Sampling_Date'].dt.year\n",
    "train_pivot = train_pivot.drop('Sampling_Date', axis=1)\n",
    "\n",
    "test_df['image_id'] = test_df['image_path'].apply(lambda x: x.split('/')[1].replace('.jpg', ''))\n",
    "test_pivot = test_df[['image_id', 'image_path', 'State', 'Species']].drop_duplicates().reset_index(drop=True)\n",
    "test_pivot['Pre_GSHH_NDVI'] = train_pivot['Pre_GSHH_NDVI'].mean()\n",
    "test_pivot['Height_Ave_cm'] = train_pivot['Height_Ave_cm'].mean()\n",
    "test_pivot['month'] = train_pivot['month'].mode()[0]\n",
    "test_pivot['year'] = train_pivot['year'].mode()[0]\n",
    "\n",
    "# Extract image features\n",
    "base_model_vgg = VGG16(weights='imagenet', include_top=False)\n",
    "def extract_image_features_vgg(img_path):\n",
    "    img = image.load_img(img_path, target_size=(224, 224))\n",
    "    x = image.img_to_array(img)\n",
    "    x = np.expand_dims(x, axis=0)\n",
    "    x = preprocess_input(x)\n",
    "    return base_model_vgg.predict(x).flatten()\n",
    "\n",
    "train_pivot['image_features'] = train_pivot['image_path'].apply(lambda x: extract_image_features_vgg(data_path / x))\n",
    "test_pivot['image_features'] = test_pivot['image_path'].apply(lambda x: extract_image_features_vgg(data_path / x))\n",
    "\n",
    "X_train_img = np.array(train_pivot['image_features'].tolist())\n",
    "X_test_img = np.array(test_pivot['image_features'].tolist())\n",
    "\n",
    "X_train_tabular = train_pivot.drop(['image_path', 'Dry_Clover_g', 'Dry_Dead_g', 'Dry_Green_g', 'Dry_Total_g', 'GDM_g', 'image_features'], axis=1)\n",
    "X_test_tabular = test_pivot.drop(['image_path', 'image_id', 'image_features'], axis=1)\n",
    "\n",
    "categorical_features = ['State', 'Species']\n",
    "encoder = OneHotEncoder(handle_unknown='ignore', sparse_output=False)\n",
    "X_train_encoded = encoder.fit_transform(X_train_tabular[categorical_features])\n",
    "X_test_encoded = encoder.transform(X_test_tabular[categorical_features])\n",
    "\n",
    "X_train_numerical = X_train_tabular.drop(categorical_features, axis=1)\n",
    "X_test_numerical = X_test_tabular.drop(categorical_features, axis=1)\n",
    "\n",
    "X_train_final = np.hstack([X_train_numerical.values, X_train_encoded, X_train_img])\n",
    "X_test_final = np.hstack([X_test_numerical.values, X_test_encoded, X_test_img])\n",
    "\n",
    "y = train_pivot[['Dry_Clover_g', 'Dry_Dead_g', 'Dry_Green_g', 'Dry_Total_g', 'GDM_g']]\n",
    "\n",
    "models = {}\n",
    "for target in y.columns:\n",
    "    print(f'Training model for {target}')\n",
    "    model = RandomForestRegressor(n_estimators=100, random_state=42, n_jobs=-1)\n",
    "    model.fit(X_train_final, y[target])\n",
    "    models[target] = model\n",
    "\n",
    "predictions = {}\n",
    "for target, model in models.items():\n",
    "    predictions[target] = model.predict(X_test_final)\n",
    "\n",
    "submission_list = []\n",
    "for i, row in test_pivot.iterrows():\n",
    "    image_id = row['image_id']\n",
    "    for target_name in y.columns:\n",
    "        sample_id = f\"{image_id}__{target_name}\"\n",
    "        prediction = predictions[target_name][i]\n",
    "        submission_list.append({'sample_id': sample_id, 'target': prediction})\n",
    "\n",
    "submission_df = pd.DataFrame(submission_list)\n",
    "submission_df.to_csv('submission_final.csv', index=False)\n",
    "\n",
    "print('Final submission file created successfully!')\n",
    "submission_df.head()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}