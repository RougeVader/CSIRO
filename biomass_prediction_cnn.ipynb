{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "from pathlib import Path\n",
    "\n",
    "data_path = Path('.')\n",
    "train_df = pd.read_csv(data_path / 'train.csv')\n",
    "test_df = pd.read_csv(data_path / 'test.csv')\n",
    "\n",
    "print('train_df shape:', train_df.shape)\n",
    "print('test_df shape:', test_df.shape)\n",
    "train_df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Pivot the training data\n",
    "train_pivot = train_df.pivot_table(index=['image_path', 'Sampling_Date', 'State', 'Species', 'Pre_GSHH_NDVI', 'Height_Ave_cm'], columns='target_name', values='target').reset_index()\n",
    "train_pivot.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Feature Engineering\n",
    "train_pivot['Sampling_Date'] = pd.to_datetime(train_pivot['Sampling_Date'])\n",
    "train_pivot['month'] = train_pivot['Sampling_Date'].dt.month\n",
    "train_pivot['year'] = train_pivot['Sampling_Date'].dt.year\n",
    "train_pivot = train_pivot.drop('Sampling_Date', axis=1)\n",
    "\n",
    "test_df['image_id'] = test_df['image_path'].apply(lambda x: x.split('/')[1].replace('.jpg', ''))\n",
    "\n",
    "# Create a test_pivot dataframe with the same columns as train_pivot for the features\n",
    "test_pivot = pd.DataFrame(test_df['image_path'].unique(), columns=['image_path'])\n",
    "test_pivot['image_id'] = test_pivot['image_path'].apply(lambda x: x.split('/')[1].replace('.jpg', ''))\n",
    "\n",
    "# For simplicity, we'll create a 'metadata' dataframe from the training data\n",
    "metadata = train_df[['State', 'Species', 'Pre_GSHH_NDVI', 'Height_Ave_cm']].drop_duplicates()\n",
    "\n",
    "# And then merge it with the test_pivot. This is a strong assumption and might not be accurate.\n",
    "# A better approach would be to find a way to link test images to their metadata.\n",
    "test_pivot['State'] = train_pivot['State'].mode()[0]\n",
    "test_pivot['Species'] = train_pivot['Species'].mode()[0]\n",
    "test_pivot['Pre_GSHH_NDVI'] = train_pivot['Pre_GSHH_NDVI'].mean()\n",
    "test_pivot['Height_Ave_cm'] = train_pivot['Height_Ave_cm'].mean()\n",
    "test_pivot['month'] = train_pivot['month'].mode()[0]\n",
    "test_pivot['year'] = train_pivot['year'].mode()[0]\n",
    "\n",
    "train_pivot.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from keras.applications.vgg16 import VGG16, preprocess_input\n",
    "from keras.preprocessing import image\n",
    "from keras.models import Model\n",
    "\n",
    "def extract_image_features(img_path):\n",
    "    img = image.load_img(img_path, target_size=(224, 224))\n",
    "    x = image.img_to_array(img)\n",
    "    x = np.expand_dims(x, axis=0)\n",
    "    x = preprocess_input(x)\n",
    "    return base_model.predict(x).flatten()\n",
    "\n",
    "base_model = VGG16(weights='imagenet', include_top=False)\n",
    "\n",
    "train_pivot['image_features'] = train_pivot['image_path'].apply(lambda x: extract_image_features(data_path / x))\n",
    "test_pivot['image_features'] = test_pivot['image_path'].apply(lambda x: extract_image_features(data_path / x))\n",
    "\n",
    "X_train_img = np.array(train_pivot['image_features'].tolist())\n",
    "X_test_img = np.array(test_pivot['image_features'].tolist())\n",
    "\n",
    "X_train_tabular = train_pivot.drop(['image_path', 'Dry_Clover_g', 'Dry_Dead_g', 'Dry_Green_g', 'Dry_Total_g', 'GDM_g', 'image_features'], axis=1)\n",
    "X_test_tabular = test_pivot.drop(['image_path', 'image_id', 'image_features'], axis=1)\n",
    "\n",
    "categorical_features = ['State', 'Species']\n",
    "encoder = OneHotEncoder(handle_unknown='ignore', sparse_output=False)\n",
    "X_train_encoded = encoder.fit_transform(X_train_tabular[categorical_features])\n",
    "X_test_encoded = encoder.transform(X_test_tabular[categorical_features])\n",
    "\n",
    "X_train_numerical = X_train_tabular.drop(categorical_features, axis=1)\n",
    "X_test_numerical = X_test_tabular.drop(categorical_features, axis=1)\n",
    "\n",
    "X_train_final = np.hstack([X_train_numerical.values, X_train_encoded, X_train_img])\n",
    "X_test_final = np.hstack([X_test_numerical.values, X_test_encoded, X_test_img])\n",
    "\n",
    "y = train_pivot[['Dry_Clover_g', 'Dry_Dead_g', 'Dry_Green_g', 'Dry_Total_g', 'GDM_g']]\n",
    "\n",
    "models = {}\n",
    "for target in y.columns:\n",
    "    print(f'Training model for {target}')\n",
    "    model = RandomForestRegressor(n_estimators=100, random_state=42, n_jobs=-1)\n",
    "    model.fit(X_train_final, y[target])\n",
    "    models[target] = model\n",
    "\n",
    "predictions = {}\n",
    "for target, model in models.items():\n",
    "    predictions[target] = model.predict(X_test_final)\n",
    "\n",
    "submission_list = []\n",
    "for i, row in test_pivot.iterrows():\n",
    "    image_id = row['image_id']\n",
    "    for target_name in y.columns:\n",
    "        sample_id = f\"{image_id}___{target_name}\"
",
    "        prediction = predictions[target_name][i]\n",
    "        submission_list.append({'sample_id': sample_id, 'target': prediction})\n",
    "\n",
    "submission_df = pd.DataFrame(submission_list)\n",
    "submission_df.to_csv('submission_cnn.csv', index=False)\n",
    "\n",
    "print('Submission file created successfully!')\n",
    "submission_df.head()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
